{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbzBJ1m9FBBb"
   },
   "source": [
    "<center><h1>2-ab: Introduction to Neural Networks</h1></center>\n",
    "\n",
    "<center><h2><a href=\"https://rdfia.github.io/\">Course link</a></h2></center>\n",
    "\n",
    "# Warning : \n",
    "# Do \"File -> Save a copy in Drive\" before you start modifying the notebook, otherwise your modifications will not be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfnKy8NB8J5e",
    "outputId": "d8c0b679-a1eb-469a-90e5-d1feb3b8ce33"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n",
    "!unzip -j 2-ab.zip\n",
    "!wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vQ_LLdx8J5b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%run 'utils-data.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48x_ha7f8J5i"
   },
   "source": [
    "# Part 1 : Forward and backward passes \"by hands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtizX1JV8J5n"
   },
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    nx, nh, ny: integers\n",
    "    out params: dictionnary\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # fill values for Wh, Wy, bh, by\n",
    "\n",
    "    params[\"Wh\"] = 0.3 * torch.randn(nh, nx)\n",
    "    params[\"Wy\"] = 0.3 * torch.randn(ny, nh)\n",
    "    params[\"bh\"] = torch.zeros(1, nh)\n",
    "    params[\"by\"] = torch.zeros(1, ny)\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk-N_Ny67yo-"
   },
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    \"\"\"\n",
    "    params: dictionnary\n",
    "    X: (n_batch, dimension)\n",
    "    \"\"\"\n",
    "    bsize = X.size(0)\n",
    "    nh = params[\"Wh\"].size(0)\n",
    "    ny = params[\"Wy\"].size(0)\n",
    "    outputs = {}\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # fill values for X, htilde, h, ytilde, yhat\n",
    "\n",
    "    outputs[\"X\"] = X\n",
    "    outputs[\"htilde\"] = torch.mm(outputs[\"X\"], params[\"Wh\"].t()) + params[\"bh\"].expand(\n",
    "        bsize, nh\n",
    "    )\n",
    "    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n",
    "    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"], params[\"Wy\"].t()) + params[\"by\"].expand(\n",
    "        bsize, ny\n",
    "    )\n",
    "    outputs[\"ytilde\"] = torch.exp(outputs[\"ytilde\"])\n",
    "    outputs[\"yhat\"] = outputs[\"ytilde\"] / torch.sum(\n",
    "        outputs[\"ytilde\"], 1, keepdim=True\n",
    "    ).expand_as(outputs[\"ytilde\"])\n",
    "\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "\n",
    "    return outputs[\"yhat\"], outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uB0A2b28NZK"
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(Yhat, Y):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    L = -torch.mean(torch.sum(Y * torch.log(Yhat), 0))\n",
    "\n",
    "    _, indsY = torch.max(Y, 1)\n",
    "    _, indsYhat = torch.max(Yhat, 1)\n",
    "    sum = torch.sum((Y * torch.log(Yhat)), 1)\n",
    "    acc = torch.sum(torch.eq(indsY, indsYhat)) / Yhat.size(0) * 100\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWJjdiFe8qi5"
   },
   "outputs": [],
   "source": [
    "def backward(params, outputs, Y):\n",
    "    bsize = Y.shape[0]\n",
    "    grads = {}\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # fill values for Wy, Wh, by, bh\n",
    "\n",
    "    grads[\"ytilde\"] = outputs[\"yhat\"] - Y\n",
    "\n",
    "    grads[\"Wy\"] = torch.mm(grads[\"ytilde\"].t(), outputs[\"h\"]) / bsize\n",
    "    grads[\"htilde\"] = (torch.mm(grads[\"ytilde\"], params[\"Wy\"])) * (\n",
    "        1 - torch.pow(outputs[\"h\"], 2)\n",
    "    )\n",
    "    grads[\"Wh\"] = torch.mm(grads[\"htilde\"].t(), outputs[\"X\"]) / bsize\n",
    "    grads[\"by\"] = torch.sum(grads[\"ytilde\"], 0).t() / bsize\n",
    "    grads[\"bh\"] = torch.sum(grads[\"htilde\"], 0).t() / bsize\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAnsISsW9CnH"
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, eta):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # update the params values\n",
    "\n",
    "    params[\"Wh\"] -= eta * grads[\"Wh\"]\n",
    "    params[\"Wy\"] -= eta * grads[\"Wy\"]\n",
    "    params[\"bh\"] -= eta * grads[\"bh\"]\n",
    "    params[\"by\"] -= eta * grads[\"by\"]\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4RSw6bd0-qUe",
    "outputId": "2ce1b761-cc78-472f-e194-d2177a741d14"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.05\n",
    "\n",
    "params = init_params(nx, nh, ny)\n",
    "\n",
    "curves = [[], [], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(300):\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "        indsBatch = range(j * Nbatch, (j + 1) * Nbatch)\n",
    "        X_train = Xtrain[indsBatch, :]\n",
    "        Y_train = Ytrain[indsBatch, :]\n",
    "\n",
    "        # write the optimization algorithm on the batch (X,Y)\n",
    "        # using the functions: forward, loss_accuracy, backward, sgd\n",
    "        Yhat_train, outputs = forward(params, X_train)\n",
    "        Ltrain, acctrain = loss_accuracy(Yhat_train, Y_train)\n",
    "        grads = backward(params, outputs, Y_train)\n",
    "        params = sgd(params, grads, eta)\n",
    "\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    Yhat_train, _ = forward(params, data.Xtrain)\n",
    "    Yhat_test, _ = forward(params, data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
    "    Ygrid, _ = forward(params, data.Xgrid)\n",
    "\n",
    "    title = \"Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})\".format(\n",
    "        iteration, acctrain, Ltrain, acctest, Ltest\n",
    "    )\n",
    "    print(title)\n",
    "    data.plot_data_with_grid(Ygrid, title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain)\n",
    "    curves[3].append(Ltest)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy / loss\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hifuW5UFA3DZ"
   },
   "source": [
    "## Global learning procedure \"by hands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrHHH5PL8J54"
   },
   "source": [
    "# Part 2 : Simplification of the backward pass with `torch.autograd`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G4q5zP0CEvB"
   },
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    nx, nh, ny: integers\n",
    "    out params: dictionnary\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # fill values for Wh, Wy, bh, by\n",
    "    params[\"Wh\"] = 0.3 * torch.randn(nh, nx, requires_grad=True)\n",
    "    params[\"Wh\"].retain_grad()\n",
    "\n",
    "    params[\"Wy\"] = 0.3 * torch.randn(ny, nh, requires_grad=True)\n",
    "    params[\"Wy\"].retain_grad()\n",
    "    \n",
    "    params[\"bh\"] = torch.zeros(1, nh, requires_grad=True)\n",
    "    params[\"bh\"].retain_grad()\n",
    "    \n",
    "    params[\"by\"] = torch.zeros(1, ny, requires_grad=True)\n",
    "    params[\"by\"].retain_grad()\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL0tSjpKCyVB"
   },
   "source": [
    "The function `forward` remains unchanged from previous part. \n",
    "\n",
    "The function `backward` is no longer used because of \"autograd\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA4ycHlfBzCK"
   },
   "outputs": [],
   "source": [
    "def sgd(params, eta):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # update the network weights\n",
    "    # warning: use torch.no_grad()\n",
    "    # and reset to zero the gradient accumulators\n",
    "    with torch.no_grad():\n",
    "        params[\"Wh\"] -= eta * params[\"Wh\"].grad\n",
    "        params[\"Wh\"].grad.zero_()\n",
    "\n",
    "        params[\"Wy\"] -= eta * params[\"Wy\"].grad\n",
    "        params[\"Wy\"].grad.zero_()\n",
    "        \n",
    "        params[\"bh\"] -= eta * params[\"bh\"].grad\n",
    "        params[\"bh\"].grad.zero_()\n",
    "        \n",
    "        params[\"by\"] -= eta * params[\"by\"].grad\n",
    "        params[\"by\"].grad.zero_()\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjgcmgQpDfOb"
   },
   "source": [
    "## Global learning procedure with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8p5oR3EqDea-",
    "outputId": "806fb47f-6d9a-4fc1-fd41-7f3b736ec585"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "params = init_params(nx, nh, ny)\n",
    "\n",
    "curves = [[], [], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(300):\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "        # write the optimization algorithm on the batch (X,Y)\n",
    "        # using the functions: forward, loss_accuracy, sgd\n",
    "        # and the backward function with autograd\n",
    "        indsBatch = range(j * Nbatch, (j + 1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch, :]\n",
    "        Yhat_train, outputs = forward(params, X)\n",
    "        Ltrain, acctrain = loss_accuracy(Yhat_train, Y)\n",
    "        Ltrain.backward(retain_graph=True)\n",
    "        params = sgd(params, eta)\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    Yhat_train, _ = forward(params, data.Xtrain)\n",
    "    Yhat_test, _ = forward(params, data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
    "    Ygrid, _ = forward(params, data.Xgrid)\n",
    "\n",
    "    title = \"Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})\".format(\n",
    "        iteration, acctrain, Ltrain, acctest, Ltest\n",
    "    )\n",
    "    print(title)\n",
    "    # detach() is used to remove the predictions from the computational graph in autograd\n",
    "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain.detach())\n",
    "    curves[3].append(Ltest.detach())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy / loss\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FV1iss68J6H"
   },
   "source": [
    "# Part 3 : Simplification of the forward pass with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6T5Uq7JEl47"
   },
   "source": [
    "`init_params` and `forward` are replaced by the `init_model` function which defines the network architecture and the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-h4r-FH8J6I"
   },
   "outputs": [],
   "source": [
    "def init_model(nx, nh, ny):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nx, nh),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(nh, ny),\n",
    "        torch.nn.Softmax(),\n",
    "    )\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "\n",
    "    return model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geE_TI96FXnl"
   },
   "outputs": [],
   "source": [
    "def loss_accuracy(loss, Yhat, Y):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # call the loss function\n",
    "    L = loss(Yhat, Y)\n",
    "    _, indsY = torch.max(Y, 1)\n",
    "    _, indsYhat = torch.max(Yhat, 1)\n",
    "    sum = torch.sum((Y * torch.log(Yhat)), 1)\n",
    "    acc = torch.sum(torch.eq(indsY, indsYhat)) / Yhat.size(0) * 100\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "\n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e93bvFiYGKnA"
   },
   "outputs": [],
   "source": [
    "def sgd(model, eta):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # update the network weights\n",
    "    # warning: use torch.no_grad()\n",
    "    # and reset to zero the gradient accumulators\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= eta * param.grad\n",
    "        model.zero_grad()\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOxBMmD4Gxtp"
   },
   "source": [
    "## Global learning procedure with autograd and `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4hMBmCNvHCLn",
    "outputId": "fda61824-c0fe-4d01-d8e5-eb8ab48b0c90"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "model, loss = init_model(nx, nh, ny)\n",
    "\n",
    "curves = [[], [], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(300):\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "        # write the optimization algorithm on the batch (X,Y)\n",
    "        # using the functions: loss_accuracy, sgd\n",
    "        # the forward with the predict method from the model\n",
    "        # and the backward function with autograd\n",
    "        indsBatch = range(j * Nbatch, (j + 1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch, :]\n",
    "        Yhat = model(X)\n",
    "        Ltrain, acctrain = loss_accuracy(loss, Yhat, Y)\n",
    "        Ltrain.backward(retain_graph=True)\n",
    "        model = sgd(model, eta)\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "    Yhat_train = model(data.Xtrain)\n",
    "    Yhat_test = model(data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
    "    Ygrid = model(data.Xgrid)\n",
    "\n",
    "    title = \"Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})\".format(\n",
    "        iteration, acctrain, Ltrain, acctest, Ltest\n",
    "    )\n",
    "    print(title)\n",
    "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain.detach() * 100)\n",
    "    curves[3].append(Ltest.detach() * 100)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy / Loss (X 100)\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoFSrQNsJCnz"
   },
   "source": [
    "# Part 4 : Simplification of the SGD with `torch.optim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8WtN9loJPqP"
   },
   "outputs": [],
   "source": [
    "def init_model(nx, nh, ny, eta):\n",
    "    #####################\n",
    "    ## Your code here  ##\n",
    "    #####################\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nx, nh),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(nh, ny),\n",
    "    )\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
    "    ####################\n",
    "    ##      END        #\n",
    "    ####################\n",
    "\n",
    "    return model, loss, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY-0rRzPJYDd"
   },
   "source": [
    "The `sgd` function is replaced by calling the `optim.zero_grad()` before the backward and `optim.step()` after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q82hCupvJxvV"
   },
   "source": [
    "## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "V9h9nINKJ1LU",
    "outputId": "2a2aeba1-f259-4001-f072-3df4411a9ed4"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "data = CirclesData()\n",
    "data.plot_data()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 10\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 10\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03\n",
    "\n",
    "model, loss, optim = init_model(nx, nh, ny, eta)\n",
    "\n",
    "curves = [[], [], [], []]\n",
    "\n",
    "# epoch\n",
    "for iteration in range(300):\n",
    "    # permute\n",
    "    perm = np.random.permutation(N)\n",
    "    Xtrain = data.Xtrain[perm, :]\n",
    "    Ytrain = data.Ytrain[perm, :]\n",
    "\n",
    "    #####################\n",
    "    ## Your code  here ##\n",
    "    #####################\n",
    "    # batches\n",
    "    for j in range(N // Nbatch):\n",
    "        # write the optimization algorithm on the batch (X,Y)\n",
    "        # using the functions: loss_accuracy\n",
    "        # the forward with the predict method from the model\n",
    "        # the backward function with autograd\n",
    "        # and then an optimization step\n",
    "        indsBatch = range(j * Nbatch, (j + 1) * Nbatch)\n",
    "        X = Xtrain[indsBatch, :]\n",
    "        Y = Ytrain[indsBatch, :]\n",
    "\n",
    "        Yhat = model(X)\n",
    "        Ltrain, acctrain = loss_accuracy(loss, Yhat, Y)\n",
    "        optim.zero_grad()\n",
    "        Ltrain.backward(retain_graph=True)\n",
    "        optim.step()\n",
    "    ####################\n",
    "    ##      FIN        #\n",
    "    ####################\n",
    "    Yhat_train = model(data.Xtrain)\n",
    "    Yhat_test = model(data.Xtest)\n",
    "    Ltrain, acctrain = loss_accuracy(loss, Yhat_train, data.Ytrain)\n",
    "    Ltest, acctest = loss_accuracy(loss, Yhat_test, data.Ytest)\n",
    "\n",
    "    title = \"Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})\".format(\n",
    "        iteration, acctrain, Ltrain, acctest, Ltest\n",
    "    )\n",
    "    print(title)\n",
    "\n",
    "    curves[0].append(acctrain)\n",
    "    curves[1].append(acctest)\n",
    "    curves[2].append(Ltrain.detach() * 100)\n",
    "    curves[3].append(Ltest.detach() * 100)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy / Loss (X 100)\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.plot(curves[0], label=\"acc. train\")\n",
    "plt.plot(curves[1], label=\"acc. test\")\n",
    "plt.plot(curves[2], label=\"loss train\")\n",
    "plt.plot(curves[3], label=\"loss test\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ts1s4JuOSaZ3"
   },
   "source": [
    "# Part 5 : MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jly9C4FCSzLP"
   },
   "source": [
    "Apply the code from previous part code to the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osrFoEr_Syi7"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "data = MNISTData()\n",
    "N = data.Xtrain.shape[0]\n",
    "Nbatch = 100\n",
    "nx = data.Xtrain.shape[1]\n",
    "nh = 100\n",
    "ny = data.Ytrain.shape[1]\n",
    "eta = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRoiGbhvmSLO"
   },
   "source": [
    "# Part 6: Bonus: SVM\n",
    "\n",
    "\n",
    "Train a SVM model on the Circles dataset.\n",
    "\n",
    "Ideas : \n",
    "- First try a linear SVM (sklearn.svm.LinearSVC dans scikit-learn). Does it work well ? Why ?\n",
    "- Then try more complex kernels (sklearn.svm.SVC). Which one is the best ? why ?\n",
    "- Does the parameter C of regularization have an impact? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWeW8siymR3g"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "data = CirclesData()\n",
    "Xtrain = data.Xtrain.numpy()\n",
    "Ytrain = data.Ytrain[:, 0].numpy()\n",
    "\n",
    "Xgrid = data.Xgrid.numpy()\n",
    "\n",
    "Xtest = data.Xtest.numpy()\n",
    "Ytest = data.Ytest[:, 0].numpy()\n",
    "\n",
    "\n",
    "def plot_svm_predictions(data, predictions):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.imshow(np.reshape(predictions, (40, 40)))\n",
    "    plt.plot(\n",
    "        data._Xtrain[data._Ytrain[:, 0] == 1, 0] * 10 + 20,\n",
    "        data._Xtrain[data._Ytrain[:, 0] == 1, 1] * 10 + 20,\n",
    "        \"bo\",\n",
    "        label=\"Train\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        data._Xtrain[data._Ytrain[:, 1] == 1, 0] * 10 + 20,\n",
    "        data._Xtrain[data._Ytrain[:, 1] == 1, 1] * 10 + 20,\n",
    "        \"ro\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        data._Xtest[data._Ytest[:, 0] == 1, 0] * 10 + 20,\n",
    "        data._Xtest[data._Ytest[:, 0] == 1, 1] * 10 + 20,\n",
    "        \"b+\",\n",
    "        label=\"Test\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        data._Xtest[data._Ytest[:, 1] == 1, 0] * 10 + 20,\n",
    "        data._Xtest[data._Ytest[:, 1] == 1, 1] * 10 + 20,\n",
    "        \"r+\",\n",
    "    )\n",
    "    plt.xlim(0, 39)\n",
    "    plt.ylim(0, 39)\n",
    "    plt.clim(0.3, 0.7)\n",
    "    plt.draw()\n",
    "    plt.pause(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "e1xcE6zbmXU1",
    "outputId": "6abc1385-f6d0-4923-d8bd-6b9f1a58cad4"
   },
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "############################\n",
    "### Your code here   #######\n",
    "### Train the SVM    #######\n",
    "## See https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "## and https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "############################\n",
    "svmlin = sklearn.svm.LinearSVC(C=0.01)\n",
    "svmlin.fit(Xtrain, Ytrain)\n",
    "svmsig = sklearn.svm.SVC(C=0.1, kernel=\"sigmoid\")\n",
    "svmsig.fit(Xtrain, Ytrain)\n",
    "svmrbf = sklearn.svm.SVC(C=0.01, kernel=\"rbf\")\n",
    "svmrbf.fit(Xtrain, Ytrain)\n",
    "svmpoly = sklearn.svm.SVC(C=100, kernel=\"poly\")\n",
    "svmpoly.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vgLl7B_3mbOs",
    "outputId": "41953ace-e117-4c02-add9-9837dbe0bdc3"
   },
   "outputs": [],
   "source": [
    "## Print results\n",
    "\n",
    "Ytest_pred = svmlin.predict(Xtest)\n",
    "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
    "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
    "Ygrid_pred = svmlin.predict(Xgrid)\n",
    "plot_svm_predictions(data, Ygrid_pred)\n",
    "\n",
    "Ytest_pred = svmsig.predict(Xtest)\n",
    "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
    "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
    "Ygrid_pred = svmsig.predict(Xgrid)\n",
    "plot_svm_predictions(data, Ygrid_pred)\n",
    "\n",
    "Ytest_pred = svmrbf.predict(Xtest)\n",
    "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
    "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
    "Ygrid_pred = svmrbf.predict(Xgrid)\n",
    "plot_svm_predictions(data, Ygrid_pred)\n",
    "\n",
    "Ytest_pred = svmpoly.predict(Xtest)\n",
    "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
    "print(f\"Accuracy : {100 * accuracy:.2f}\")\n",
    "Ygrid_pred = svmpoly.predict(Xgrid)\n",
    "plot_svm_predictions(data, Ygrid_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
